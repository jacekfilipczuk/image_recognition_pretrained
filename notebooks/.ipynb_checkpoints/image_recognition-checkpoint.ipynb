{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:34.659681Z",
     "start_time": "2019-09-09T13:15:34.371237Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import models, datasets\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:34.662632Z",
     "start_time": "2019-09-09T13:15:34.660968Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:34.964722Z",
     "start_time": "2019-09-09T13:15:34.961641Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom class to return image path in the iterator\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "# # EXAMPLE USAGE:\n",
    "# # instantiate the dataset and dataloader\n",
    "# data_dir = \"your/data_dir/here\"\n",
    "# dataset = ImageFolderWithPaths(data_dir) # our custom dataset\n",
    "# dataloader = torch.utils.DataLoader(dataset)\n",
    "\n",
    "# # iterate over data\n",
    "# for inputs, labels, paths in dataloader:\n",
    "#     # use the above variables freely\n",
    "#     print(inputs, labels, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:35.770168Z",
     "start_time": "2019-09-09T13:15:35.767615Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([            \n",
    " transforms.Resize(299),                                    \n",
    " transforms.ToTensor(),                     \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:36.530489Z",
     "start_time": "2019-09-09T13:15:36.525071Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/imagenet1000.txt\") as f:\n",
    "    inception_classes = eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:37.617516Z",
     "start_time": "2019-09-09T13:15:37.615457Z"
    }
   },
   "outputs": [],
   "source": [
    "datadir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:15:38.663051Z",
     "start_time": "2019-09-09T13:15:38.660155Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = ImageFolderWithPaths(datadir,transform=transform)\n",
    "test_data_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:19:58.020835Z",
     "start_time": "2019-09-09T13:19:56.184038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ready to predict\n"
     ]
    }
   ],
   "source": [
    "inception = models.inception_v3(pretrained=True)\n",
    "inception.eval()\n",
    "print(\"model ready to predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:16:44.205106Z",
     "start_time": "2019-09-09T13:16:43.450781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.67it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for data in tqdm(test_data_loader):\n",
    "        inputs, label, path = data        \n",
    "        outputs.append((inception(inputs),label, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:16:57.508566Z",
     "start_time": "2019-09-09T13:16:57.503225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class id:  394 sturgeon 21.342702865600586\n",
      "class id:  532 dining table, board 76.04572296142578\n",
      "class id:  665 moped 16.894296646118164\n",
      "class id:  670 motor scooter, scooter 36.5108642578125\n",
      "class id:  665 moped 20.387617111206055\n"
     ]
    }
   ],
   "source": [
    "for result in outputs:\n",
    "    out, label, path = result\n",
    "    _, index = torch.max(out, 1)\n",
    " \n",
    "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "\n",
    "    print(\"class id: \",index[0].item(), inception_classes[index[0].item()], percentage[index[0].item()].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:18:31.109540Z",
     "start_time": "2019-09-09T13:18:31.107175Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = \"../data/1/kawasaki.jpg\"\n",
    "img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:18:31.887473Z",
     "start_time": "2019-09-09T13:18:31.862128Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_t = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:18:34.585637Z",
     "start_time": "2019-09-09T13:18:32.570171Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "inception = models.inception_v3(pretrained=True)\n",
    "inception.eval()\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "out = inception(batch_t)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:18:53.410272Z",
     "start_time": "2019-09-09T13:18:53.398963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moped 20.387617111206055\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.max(out, 1)\n",
    " \n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    " \n",
    "print(inception_classes[index[0].item()], percentage[index[0].item()].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:19:21.520487Z",
     "start_time": "2019-09-09T13:19:21.514807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moped', 20.387617111206055),\n",
       " ('disk brake, disc brake', 5.139692306518555),\n",
       " ('motor scooter, scooter', 4.513574600219727),\n",
       " ('crash helmet', 0.9927563667297363),\n",
       " ('mountain bike, all-terrain bike, off-roader', 0.7166666984558105)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = torch.sort(out, descending=True)\n",
    "[(inception_classes[idx.item()], percentage[idx.item()].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
